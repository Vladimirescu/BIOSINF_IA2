{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb51e69-1ae9-48f7-9103-27f93d2cf1f0",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1528aa6-84b5-422c-96e6-664eb6ee7ed6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "- [Theory](#theory)\n",
    "- [Text Classification](#app-1)\n",
    "- [Next Frame Prediction](#app-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd427c-979a-4d3a-a19a-fc37c7a19e92",
   "metadata": {},
   "source": [
    "## **Theory** <a class=\"anchor\" id=\"theory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3745445-ad45-4f4e-a738-29a43f5dd845",
   "metadata": {},
   "source": [
    "## RNNs - generalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657fd5e-c173-4226-a51c-fb1448878425",
   "metadata": {},
   "source": [
    "Given a sequence of $T \\geq 2$ temporal states $x_0, x_2, \\dots, x_{T-1}$, with $x_i \\in \\mathbb{R}^d$, $i\\in \\{0, T-1\\}$, we want to extract task-related information from this sequence without \"breaking\" the temporal dimension. While MLPs clearly don't have a chance, CNNs may be able to grasp some temporal info by sliding a window along temporal dimension. However, given that the ordering of elements matter, *causal convolution* has to be applied s.t. the output of the current step doesn't depend on future states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889f27f",
   "metadata": {},
   "source": [
    "In standard CNNs the output $y_i$ would depend on the following input context:\n",
    "$$[x_{i - \\lfloor K/2 \\rfloor}, x_{i - \\lfloor K/2 \\rfloor + 1}, \\dots, x_i, \\dots, x_{i + \\lfloor K/2 \\rfloor - 1}, x_{i + \\lfloor K/2 \\rfloor}]$$\n",
    "while in causal convolution the input context would be:\n",
    "$$[x_{i - K - 1}, x_{i - K -2}, \\dots, x_{i-1}, x_i]$$\n",
    ", where $K \\geq 3, K\\%2 = 1$ represents the kernel/filter length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2bd64",
   "metadata": {},
   "source": [
    "Long-term dependencies that may arise in the sequence $\\{x_i\\}_{i=1}^T$ would be lost due to the limiting context length $K$. On the other hand, increasing $K$ leads to a higher memory and computational cost, along with the possibility of bypassing short-term information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47299e0a-b2c9-474b-a629-cc7f9ba2768f",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks (RNNs)** are a class of NNs tailored to temporal/sequential data, step-by-step processing each input $x_t$ using the same set of parameters, taking into account the *context* of previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1fce6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/rnn.png\" alt=\"description\" width=\"1000\"/>\n",
    "  <figcaption>Diagram of a 1-layer RNN</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5134a-e1b5-4f11-a58e-28dfe48609c4",
   "metadata": {},
   "source": [
    "**The same RNN cell** is used at each timestep, receiving the current input $x_i$ and previous state $h_{i-1}$ ($+$ any additional feature vector), and producing the current output $h_i$. In this way, multiple such RNN cells can be stacked, forming a multi-layer RNN, where upper layers receive as input the lower layers' outputs $h_i$ (the above figure corresponds to a single-layer RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0901f7-15a8-49d4-a9eb-4f5b192b775a",
   "metadata": {},
   "source": [
    "Of course, there are several disadvantages when using classic RNNs:\n",
    "- Long-term dependencies may still be lost due to the continous fading of relevant info, since a single cell cannot grasp all history\n",
    "- Lack of paralellism - we have to wait for previous state to finish computation (causality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a885980-9a4a-43df-ba25-acd11665c63d",
   "metadata": {},
   "source": [
    "## The **LSTM** cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195194a2-e2c5-486f-9218-9cc5903f94a9",
   "metadata": {},
   "source": [
    "[Long Short Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) (LSTM) cells have been introduced to mitigate the problem of forgetting long-term dependencies in classic RNN cells. \n",
    "\n",
    "Their main component is the **cell state**, an additional recurrent feature vector that passes through all timestamps, having minor influence on each output $\\rightarrow$ persists over multiple timesteps $\\rightarrow$ encapsulates long-term past states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6d8ea",
   "metadata": {},
   "source": [
    "Each LSTM cell is defined by 4 interconnected computational sub-modules:\n",
    "1. *Forget Gate*:  $\\{h_{t-1}, x_t\\} \\rightarrow$ mask $f_t$ representing **what to retain** from previous cell state $C_{t-1}$\n",
    "\n",
    "2. *Information Gate*: $\\{h_{t-1}, x_t\\} \\rightarrow$ mask $i_t$, along with new cell state candidates $\\tilde{C}_{t-1}$. Mask $i_t$ decides what to keep from current candidates $\\tilde{C}_{t-1}$ \n",
    "\n",
    "3. *State Update* (**Cell State**): cell state update based on masks $\\{f_t, i_t\\}$\n",
    "\n",
    "4. *Output Gate*: computes the final features representative for the current timestep. Roughly speking, this gate decides which elements from the cell state are going to the output at the current timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d103ef",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/lstm.png\" alt=\"description\" width=\"1000\"/>\n",
    "  <figcaption>Information flow in a LSTM cell, for a single timestep.</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2d907-a8db-47a3-bdca-8dab5b74d902",
   "metadata": {},
   "source": [
    "### Other variants:\n",
    "- Bidirectional LSTM\n",
    "- [ConvLSTM](https://proceedings.neurips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html)\n",
    "- [Self-Attention LSTM](https://ojs.aaai.org/index.php/AAAI/article/view/6819)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be96dec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fe951-89fe-4b88-a23d-93f91870a100",
   "metadata": {},
   "source": [
    "## *{Self, Cross}* - **Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d1211",
   "metadata": {},
   "source": [
    "Although not neccessarely related to recurrent stuff, **attention mechanisms** have been utilised in various domains in order to enforce the representational power through similarity matching strategies between different parts of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7818d-dd4b-4c25-a4c9-bdeacd8f59bc",
   "metadata": {},
   "source": [
    "Neural machine translation has been among the first fields working with sequential data to benefit from [Self-Attention Mechanisms](https://arxiv.org/pdf/1409.0473), showing that using a fixed-length vector representation between an encoder and a deocder comes with a significant bottleneck. In contrast, self-attention allowed a model to automatically (soft-)search for parts of a source sentence relevant for predicting a target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c9068-23ed-4074-8bf4-0ebc70571a43",
   "metadata": {},
   "source": [
    "The **multi-head self-attention** works as follows:\n",
    "\n",
    "1. For each head, the input sequential data $X\\in\\mathbb{R}^{T \\times D}$, with $T$ being the temporal dimension (or any multide of dimensions on which we can define some \"ordering\"), is linearly transformed into **query / key / value** tensors:\n",
    "$$Q_h \\gets X W_h^{Q}, \\quad K_h \\gets X W_h^{K}, \\quad V_h \\gets X W_h^{V}$$ \n",
    "$$W_h^{Q}, W_h^{K}, W_h^{V} \\in \\mathbb{R}^{D \\times D'}$$\n",
    "$$Q, K, V \\in \\mathbb{R}^{T\\times D'}$$\n",
    "$$h \\in \\{1, \\dots, n_{heads}\\}$$\n",
    "\n",
    "2. Similarity scores are computed between keys and queries *Softmax*:\n",
    "$$A_h \\gets \\text{\\texttt{softmax}}(Q_h K_h^T), \\quad A_h \\in [0, 1]^{T\\times T}$$\n",
    "\n",
    "3. These scores are used to mix the temporal entries of $V_h$, computing at each timestep a convex combination of the previous values:\n",
    "$$Z_h \\gets A_h V_h, \\quad Z_h \\in \\mathbb{R}^{T\\times D}$$\n",
    "\n",
    "4. All outputs $Z_h$ from each head $h$ are concatenated and multiplied by a final output matrix $W_o \\in \\mathbb{R}^{D' \\cdot n_{heads} \\times D}$, resulting the final features $Z$:\n",
    "$$Z \\gets \\big[Z_1 \\Vert Z_2 \\Vert \\dots \\Vert Z_{n_{heads}}\\big] W_o, \\quad Z \\in \\mathbb{R}^{T \\times D}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49747722",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/self_att.png\" alt=\"description\" width=\"1000\"/>\n",
    "  <figcaption><b>Left: </b>Computing Q, K, V. <b>Right: </b>Multi-head self-attention. Sub-images modified from <a href=\"https://jalammar.github.io/illustrated-transformer/\">here</a></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b305d5",
   "metadata": {},
   "source": [
    "(Multi-Head) **Cross-Attention** differs in that two temporal sources are analysed (not neccessarely having equal $T$), and between which attention scores $A_h$ are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94edc4f-c06e-4125-bada-eed1ab7d8331",
   "metadata": {},
   "source": [
    "Cross-Attention has been mostly used in multi-modal learning, computing similarities between encoded modalities in order to retrieve relevant responses. Most notably, [Contrastive Language-Image Pretraining](CLIP) combines text and images to create zero-shot classifiers. Another example is [CrossViT](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_CrossViT_Cross-Attention_Multi-Scale_Vision_Transformer_for_Image_Classification_ICCV_2021_paper.pdf) which attends to features resulted from two transformer encoders, merging them into a multi-scale representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f17a68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f183d49-7c87-4f4c-83bd-923ed6bb500e",
   "metadata": {},
   "source": [
    "## **Movie review classification** <a class=\"anchor\" id=\"app-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80933c31-4579-4fca-a5dd-da06a0fe93b7",
   "metadata": {},
   "source": [
    "We'll use `torchtext` package to download and prepare our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e79fd83-9543-4c1b-a61c-4bc022c8d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f25de1-52ac-4d27-a1c6-79657d373c8d",
   "metadata": {},
   "source": [
    "Install `torchtext` without overriding the current `torch` install. Take at look at [this table](https://pypi.org/project/torchtext/) and [these releases](https://github.com/pytorch/text/releases) to make sure you install the right `torchtext` version for your current `torch` installation. You'll also need `torchdata` - check out [its releases](https://github.com/pytorch/data/releases) to see which version you need. `poratlocker==2.8.2` is required, according to [this fix](https://github.com/pytorch/text/issues/2172#issuecomment-1808401332)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62e9040-f771-47d1-b602-04367e9c94e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.15.2 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: torchdata==0.6.1 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (0.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torchtext==0.15.2) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torchtext==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.1 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torchtext==0.15.2) (2.0.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torchtext==0.15.2) (1.24.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torchdata==0.6.1) (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torch==2.0.1->torchtext==0.15.2) (4.13.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
      "Requirement already satisfied: portalocker==2.8.2 in /opt/anaconda3/envs/ia2/lib/python3.10/site-packages (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.15.2 torchdata==0.6.1 \n",
    "!pip install portalocker==2.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99c67",
   "metadata": {},
   "source": [
    "Load the **SST2** dataset, containing pairs of `sentence, {0, 1} labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351fd54f-fb2e-4452-a346-999ee96983cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import SST2\n",
    "\n",
    "train_ds = SST2(root=\"data/\", split=\"train\")\n",
    "test_ds = SST2(root=\"data/\", split=\"dev\")\n",
    "\n",
    "label_names = {\n",
    "    0: \"bad\",\n",
    "    1: \"good\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4c689f-e5f3-4175-b0b5-7d9dca0de9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class y=1 (good): it 's a charming and often affecting journey .\n",
      "Class y=0 (bad): unflinchingly bleak and desperate\n",
      "Class y=1 (good): allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .\n",
      "Class y=1 (good): the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .\n",
      "Class y=0 (bad): it 's slow -- very , very slow .\n",
      "Class y=0 (bad): a sometimes tedious film .\n"
     ]
    }
   ],
   "source": [
    "seen_classes = {}\n",
    "max_per_class_samples = 3\n",
    "\n",
    "for sample in iter(test_ds):\n",
    "    x, y = sample\n",
    "\n",
    "    if y not in seen_classes.keys():\n",
    "        print(f\"Class y={y} ({label_names[y]}): {x}\")\n",
    "        seen_classes[y] = 1\n",
    "    else:\n",
    "        if seen_classes[y] < max_per_class_samples:\n",
    "            print(f\"Class y={y} ({label_names[y]}): {x}\")\n",
    "            seen_classes[y] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5af4d-535f-44b1-8fb3-08a0493a7a44",
   "metadata": {},
   "source": [
    "## Sentence Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54bcfd-9463-49e9-872c-633a08f1b3af",
   "metadata": {},
   "source": [
    "We'll pre-process each sequence in the following manner:\n",
    "$$\\text{Sentence} \\xrightarrow{\\text{Tokenizer}} [\\text{Words}] \\xrightarrow{Vocabulary} [\\text{Word Indexes}]$$\n",
    "For that, we'll need a basic english sentence (word) breaker, and a vocabulary to assigning an integer index to each unique word in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a136e8-0401-45b6-95e4-490bcbac6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(ds):\n",
    "    for text, y in ds:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_ds), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62b7bea-3882-4564-8fe2-8296c59b127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-sentence: good \\ it 's a charming and often affecting journey .\n",
      "Tokenized sequence:  ['it', \"'\", 's', 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "Vocabulary indices for the tokenized sequence:  [13, 7, 9, 3, 296, 4, 144, 1411, 596, 6]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(test_ds))\n",
    "\n",
    "print(f\"label-sentence: {label_names[y]} \\ {x}\")\n",
    "print(\"Tokenized sequence: \", tokenizer(x))\n",
    "print(\"Vocabulary indices for the tokenized sequence: \", vocab(tokenizer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd132e",
   "metadata": {},
   "source": [
    "Since we cannot directly stack multiple tokenized sequences into a `(batch_size, n_words)` tensor of indices, we'll need to pad each sentence with `<unk>` characters until the largest n.o. words in a batch is reached.\n",
    "\n",
    "We'll do this by passing a custom `collate_fn` to the dataloaders, which will automatically perform this operation on each extracted batch of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3bcf15-707b-471d-9192-9eb1c0e3ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# This will be applied over each sentence -> just like the above example\n",
    "text_processor = lambda x: vocab(tokenizer(x)) \n",
    "label_processor = lambda y: int(y)\n",
    "\n",
    "# This will be applied over each extracted batch of sentences\n",
    "def collate_fn(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_processor(_label))\n",
    "        processed_text = torch.LongTensor(text_processor(_text))\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(len(processed_text))\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<unk>\"]) # returns a (B, T_max) tensor of indices\n",
    "    length_list = torch.LongTensor(length_list)\n",
    "\n",
    "    # need to sort in descending order - s.t. the first element has the highest length in the batch\n",
    "    # this is useful for what will follow in the training phase\n",
    "    length_list, perm_idx = length_list.sort(0, descending=True)\n",
    "    text_list = text_list[perm_idx]\n",
    "    label_list = label_list[perm_idx]\n",
    "    \n",
    "    return text_list.to(device), label_list.to(device), length_list.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5dc914f-ec6f-4011-8366-3bc46099d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Max size in batch:  35\n",
      "Batch tensor shape:  torch.Size([64, 35])\n",
      "All lenghts in batch:  tensor([35, 25, 25, 25, 21, 20, 19, 18, 18, 18, 17, 17, 16, 15, 13, 12, 11, 11,\n",
      "        10, 10, 10,  9,  8,  8,  8,  8,  8,  8,  7,  7,  7,  7,  6,  6,  6,  6,\n",
      "         5,  5,  5,  5,  5,  4,  4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  2,  2,  2,  2,  2,  1,  1,  1,  1])\n",
      "First 3 examples in batch:\n",
      "->  [  10    1  157    5   20  702   78   22  303    8 4480   68  457   25\n",
      "    1  634   12 1196    5   22    2   32   18 4507    2   32   18 3898\n",
      "    2   32   18  679 5348 1825    6]\n",
      "->  [  150  4723   173    35   233  5142    92     2    42  2907 12467   238\n",
      "     2    23    13     7     9   509    16   401   507     8   168    32\n",
      "    95     0     0     0     0     0     0     0     0     0     0]\n",
      "->  [  40  923   12    3 4301 6637    2 7732    3 1538    5 1020 5327 4975\n",
      "   25   36  131    2    4 1882 3623   22  116  550 7933    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x, y, lengths = batch\n",
    "\n",
    "    bs, max_size = x.shape[0], x.shape[1]\n",
    "\n",
    "    print(\"Batch size: \", bs)\n",
    "    print(\"Max size in batch: \", max_size)\n",
    "    print(\"Batch tensor shape: \", x.shape)\n",
    "    print(\"All lenghts in batch: \", lengths)\n",
    "\n",
    "    print(\"First 3 examples in batch:\")\n",
    "    for i in range(3):\n",
    "        print(\"-> \", x[i].cpu().numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c44a23-43e4-46bb-a752-6e7d78cb1a83",
   "metadata": {},
   "source": [
    "A `nn.Embedding` layer is used to convert these integer indexes into float tensor representations. An embedding layer works as a lookup table, mapping word indexes to real-valued tensors, which are optimized during training along with all the other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9b0d5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/embedd.png\" alt=\"description\" width=\"800\"/>\n",
    "  <figcaption>Simplified diagram of sentence embedding.</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fbf50a5-6eb9-4186-bf3d-f317f35ccb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch \\ Seq.Len \\ Embedding dim: 64 \\ 35 \\ 50\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "example_embedd = nn.Embedding(len(vocab), 50)\n",
    "embedded_x = example_embedd(x.cpu()) # computes the embedding separately and independently for each word (index)\n",
    "bs, seq, emb = embedded_x.shape\n",
    "print(f\"Batch \\ Seq.Len \\ Embedding dim: {bs} \\ {seq} \\ {emb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2b4c9-f311-4059-a60d-ff5be362731e",
   "metadata": {},
   "source": [
    "## Build and Train Embeddings + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac939a-0b98-4f92-b878-9cd5cba3dbce",
   "metadata": {},
   "source": [
    "In order to efficiently work with batches containing variable-length data (effective length) we'll need to `pad-pack` each batch s.t. a batch is presented to the LSTM as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da3840",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/pack_padded.png\" alt=\"description\" width=\"1200\"/>\n",
    "  <figcaption>Pack-padding strategy for training on variable-sized sentences.</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2115cdfc-7a7e-4bf9-b260-04c243293179",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedd_size, num_layers, lstm_hidden_dim, n_classes, \n",
    "                 bidirectional=False, predict_on_output=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.embedd_size = embedd_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedd_size, padding_idx=0, scale_grad_by_freq=False,\n",
    "                                     max_norm=1.0) # this receives indices and returns float tensor representations\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(embedd_size, lstm_hidden_dim, \n",
    "                            dropout=0., num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # We can choose between predicting on the output tensor, or on the hidden state\n",
    "        if predict_on_output:\n",
    "            if bidirectional:\n",
    "                # In the bidirectional case, we'll have two two outputs - one for ->, another for <-\n",
    "                self.out_dim = lstm_hidden_dim * 2\n",
    "            else:\n",
    "                self.out_dim = lstm_hidden_dim\n",
    "        else:\n",
    "            # predict on the concatenation of last hidden states, from each layer\n",
    "            if bidirectional:\n",
    "                self.out_dim = lstm_hidden_dim * num_layers * 2\n",
    "            else:\n",
    "                self.out_dim = lstm_hidden_dim * num_layers\n",
    "\n",
    "        self.out_classification = nn.Sequential(\n",
    "            nn.Linear(self.out_dim, n_classes) \n",
    "        )\n",
    "        \n",
    "        self.predict_on_output = predict_on_output\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        # need to pack-padd embeddings, in order to exclude padded values from computation in LSTM\n",
    "        embeddings_ = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu().numpy(), batch_first=True).to(device)\n",
    "\n",
    "        # hidden and cell states will be returned for each LSTM layer - (num_layers, batch_size, lstm_hidden_dim)\n",
    "        out, (hidden_state, cell_state) = self.lstm(embeddings_)\n",
    "\n",
    "        # unpack padded output sequence\n",
    "        out_, lens_unpacked  = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        if self.predict_on_output:\n",
    "            # take the output for each element in the batch, at the index of len_sequence - 1 (last word)\n",
    "            preds = self.out_classification(\n",
    "                        torch.gather(\n",
    "                            out_, dim=1, \n",
    "                            index=(lens_unpacked - 1).view(-1, 1).unsqueeze(2).expand(-1, -1, out_.size(-1)).to(out_.device)\n",
    "                        ).squeeze()\n",
    "                    )\n",
    "        else:\n",
    "            # Classify on the concatenation of hidden_states from each layer\n",
    "            preds = self.out_classification(\n",
    "                        torch.cat([hidden_state[i] for i in range(self.num_layers)], dim=1)\n",
    "                    )  \n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69326378",
   "metadata": {},
   "source": [
    "Define hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21fdff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_size = 50\n",
    "num_layers = 2\n",
    "lstm_hidden_dim = 32\n",
    "bidirectional = False\n",
    "predict_on_output = True\n",
    "n_classes = 2\n",
    "\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "l2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4e851a-bb58-4034-86c8-e8892c5bc480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape:  torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifier(len(vocab), \n",
    "                      embedd_size, \n",
    "                      num_layers, \n",
    "                      lstm_hidden_dim, \n",
    "                      n_classes,\n",
    "                      bidirectional=bidirectional,\n",
    "                      predict_on_output=predict_on_output).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "folder_path = \"models/sst2/\"\n",
    "file_name = \"model.pth\"\n",
    "\n",
    "print(\"Model output shape: \", model(x, lengths).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b11ee",
   "metadata": {},
   "source": [
    "After training, you'll observe some overfitting - you can play with $\\ell_2$-regularization, but be careful not to assign a too big weighting factor, otherwise it will not learn at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37b45638-435c-4668-b14f-c6b424cefcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/ia2/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "1053it [00:15, 69.57it/s]\n",
      "14it [00:00, 119.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: train_loss=0.21116176960460933 train_acc=0.9201918365528813 test_loss=0.466987030846732 test_acc=0.823394495412844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1053it [00:15, 67.53it/s]\n",
      "14it [00:00, 122.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: train_loss=0.17473907431928973 train_acc=0.9344756418061144 test_loss=0.5304145451102938 test_acc=0.8222477064220184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1053it [00:15, 69.56it/s]\n",
      "14it [00:00, 127.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: train_loss=0.14936892355014797 train_acc=0.9441862536934476 test_loss=0.5338231261287417 test_acc=0.823394495412844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1053it [00:15, 69.47it/s]\n",
      "14it [00:00, 128.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: train_loss=0.12984908288141317 train_acc=0.950511514647582 test_loss=0.6557638453585761 test_acc=0.8165137614678899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1053it [00:15, 68.14it/s]\n",
      "14it [00:00, 126.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: train_loss=0.11394708342885586 train_acc=0.956228006354957 test_loss=0.7081157394817897 test_acc=0.8119266055045872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch_utils\n",
    "importlib.reload(torch_utils)\n",
    "\n",
    "train_losses, test_losses = torch_utils.train_loop(\n",
    "    model, train_loader, optimizer, criterion, epochs, \n",
    "    test_loader=test_loader, \n",
    "    device=device, \n",
    "    folder_path=folder_path, \n",
    "    file_name=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fea1360-2d9d-45fe-a7d4-12a620e690ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c913e66a08544ef946fc38fced1344a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Input:', layout=Layout(width='1000px'), placeholder='Type here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09caa8f0dd214982a6b7ede057efece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(folder_path, file_name))[\"state_dict\"]\n",
    ")\n",
    "\n",
    "input_text = widgets.Text(placeholder='Type here...', description='Input:', layout = widgets.Layout(width='1000px'))\n",
    "output_text = widgets.Output()\n",
    "\n",
    "def predict_and_update(change):\n",
    "    with output_text:\n",
    "        if len(change.new) > 0:\n",
    "            clear_output()\n",
    "            sentence = change.new\n",
    "            length = len(tokenizer(sentence))\n",
    "            tokens = torch.LongTensor(text_processor(sentence))[None, ...].to(device)\n",
    "            length = torch.LongTensor([length]).to(device)\n",
    "            \n",
    "            print('Prediction:', label_names[model(tokens, length).argmax().item()])\n",
    "\n",
    "input_text.observe(predict_and_update, names='value')\n",
    "\n",
    "display(input_text)\n",
    "display(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f013636-60a7-4d68-abdf-0e99454fe2a8",
   "metadata": {},
   "source": [
    "### Analysing word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec0b5d-79bf-492e-8501-7937c3fcf9ca",
   "metadata": {},
   "source": [
    "The `nn.Embedding` layer has optimized a $W \\in \\mathbb{R}^{|\\texttt{Vocab}| \\times \\texttt{embedd\\_size}}$ matrix containing representations for each word in our vocabulary. We can compute distances between the embedding of a word and each entry in the matrix to get the most similar or least similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f8f7e88-b128-492c-b65d-22419d60323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(k, emb_word, vocab, embeddings, device, most_similar=True, distance=\"l2\", lookup_token=None, return_res=False):\n",
    "    if distance == \"l2\":\n",
    "        distances = torch.norm(embeddings - emb_word, p=2, dim=1)\n",
    "    elif distance == \"cosine\":\n",
    "        distances = 1 - torch.nn.functional.cosine_similarity(emb_word, embeddings, dim=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance {distance}.\")\n",
    "    \n",
    "    idx_sorted = torch.argsort(distances)\n",
    "\n",
    "    if most_similar:\n",
    "        if return_res:\n",
    "            if lookup_token:\n",
    "                return [lookup_token(idx_sorted[i + 1]) for i in range(k)]\n",
    "            else:\n",
    "                return [vocab.lookup_token(idx_sorted[i + 1]) for i in range(k)]\n",
    "        else:\n",
    "            print(f\"Top k similar ({distance}): \", end=\" \")\n",
    "            for i in range(k):\n",
    "                if lookup_token:\n",
    "                    print(lookup_token(idx_sorted[i + 1]), end=\", \")\n",
    "                else:\n",
    "                    print(vocab.lookup_token(idx_sorted[i + 1]), end=\", \") # if i == 0, it would return that actual word\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        if return_res:\n",
    "            if lookup_token:\n",
    "                return [lookup_token(idx_sorted[-(i + 1)]) for i in range(k)]\n",
    "            else:\n",
    "                return [vocab.lookup_token(idx_sorted[-(i + 1)]) for i in range(k)]\n",
    "        else:\n",
    "            print(f\"Top k disimilar ({distance}): \", end=\" \")\n",
    "            for i in range(k):\n",
    "                if lookup_token:\n",
    "                    print(lookup_token(idx_sorted[-(i + 1)]), end=\", \") \n",
    "                else:\n",
    "                    print(vocab.lookup_token(idx_sorted[-(i + 1)]), end=\", \") # if i == 0, it would return that actual word    \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a073afb-94c7-421e-bf01-ef92811cfea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k similar (l2):  inhale, virulent, meanspirited, flag, sitcom-worthy, \n",
      "\n",
      "Top k disimilar (l2):  brush, compulsively, forceful, sublime, head-turner, \n",
      "\n",
      "Top k similar (cosine):  plain, soliloquies, vulgar, market, kirkegaard, \n",
      "\n",
      "Top k disimilar (cosine):  rewarded, superb, unflinching, definitely, department, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"desperate\"\n",
    "idx_word = vocab[word]\n",
    "emb_word = model.embedding(torch.LongTensor([idx_word]).to(device))\n",
    "k = 5\n",
    "\n",
    "get_top_k(k, emb_word, vocab, model.embedding.weight, device)\n",
    "get_top_k(k, emb_word, vocab, model.embedding.weight, device, most_similar=False)\n",
    "get_top_k(k, emb_word, vocab, model.embedding.weight, device, distance=\"cosine\")\n",
    "get_top_k(k, emb_word, vocab, model.embedding.weight, device, most_similar=False, distance=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6a8b671-f4de-4ae5-9bca-ebe577110a0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOVE is to ROMANCE as HATE is to ['MISSED', 'BAD', 'WINDS']\n",
      "KILLING is to ACTION as LAUGHING is to ['SINCE', 'ATTRACTIVE', 'THROUGHOUT']\n",
      "MAN is to CHEF as WOMAN is to ['LITTLE-REMEMBERED', 'WHY', 'ANOTHER']\n",
      "MAN is to DOCTOR as WOMAN is to ['CONVENIENCE', 'FLUX', 'JOSE']\n"
     ]
    }
   ],
   "source": [
    "analogies_to_explore = [\n",
    "    (\"love\", \"romance\", \"hate\"),\n",
    "    (\"killing\", \"action\", \"laughing\"),\n",
    "    (\"man\", \"chef\", \"woman\"),  \n",
    "    (\"man\", \"doctor\", \"woman\")\n",
    "]\n",
    "\n",
    "for analogy in analogies_to_explore:\n",
    "    word_a, word_b, word_c = analogy\n",
    "    idx_word_a, idx_word_b, idx_word_c = vocab[word_a], vocab[word_b], vocab[word_c]\n",
    "    \n",
    "    emb_word_a = model.embedding(torch.LongTensor([idx_word_a]).to(device))\n",
    "    emb_word_b = model.embedding(torch.LongTensor([idx_word_b]).to(device))\n",
    "    emb_word_c = model.embedding(torch.LongTensor([idx_word_c]).to(device))\n",
    "\n",
    "    new_embed = emb_word_b - emb_word_a + emb_word_c\n",
    "    \n",
    "    result = get_top_k(3, new_embed, vocab, model.embedding.weight, device, distance=\"cosine\", return_res=True)\n",
    "    result = list(map(str.upper, result))\n",
    "    \n",
    "    if result:\n",
    "        print(f\"{word_a.upper()} is to {word_b.upper()} as {word_c.upper()} is to {result}\")\n",
    "    else:\n",
    "        print(\"Some words in the analogy are not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8ba05",
   "metadata": {},
   "source": [
    "Things that have affected the quality of the above results:\n",
    "- Vocabulary was too simple - only words and phrases related to a single subject (i.e. movie reviews)\n",
    "- The classification task is not too broad - it doesn't help that much in learning meaningful relationships between embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724030da-76d5-402a-b7e8-622fe3b18aa9",
   "metadata": {},
   "source": [
    "### Compare with [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055b7f",
   "metadata": {},
   "source": [
    "[Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) is an algorithm for learning vector representations, just like jointly training a `nn.Embedding`layer along with other sub-modules in a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e286c19-1ffc-437d-bf7e-4c01249c9d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k similar (l2):  desperately, vain, wanting, trouble, weary, \n",
      "\n",
      "Top k disimilar (l2):  non-families, 202-383-7824, 20003, www.star, officership, \n",
      "\n",
      "Top k similar (cosine):  desperately, trouble, wanting, vain, weary, \n",
      "\n",
      "Top k disimilar (cosine):  landolt, preus, icct, yisheng, daojiong, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "word = \"desperate\"\n",
    "emb_word = glove[word]\n",
    "k = 5\n",
    "\n",
    "get_top_k(k, emb_word, None, glove.vectors, device, lookup_token=lambda idx: glove.itos[idx])\n",
    "get_top_k(k, emb_word, None, glove.vectors, device, most_similar=False, lookup_token=lambda idx: glove.itos[idx])\n",
    "get_top_k(k, emb_word, None, glove.vectors, device, distance=\"cosine\", lookup_token=lambda idx: glove.itos[idx])\n",
    "get_top_k(k, emb_word, None, glove.vectors, device, most_similar=False, distance=\"cosine\", lookup_token=lambda idx: glove.itos[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de510817-2b49-4111-9efa-d3f807a8b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOVE is to ROMANCE as HATE is to ['STEREOTYPING', 'HATE', 'MONGERING']\n",
      "KILLING is to ACTION as LAUGHING is to ['LAUGHING', 'BIT', 'TUNE']\n",
      "MAN is to CHEF as WOMAN is to ['WAITRESS', 'DECORATOR', 'STYLIST']\n",
      "MAN is to DOCTOR as WOMAN is to ['NURSE', 'CHILD', 'WOMAN']\n"
     ]
    }
   ],
   "source": [
    "for analogy in analogies_to_explore:\n",
    "    word_a, word_b, word_c = analogy\n",
    "    emb_word_a = glove[word_a]\n",
    "    emb_word_b = glove[word_b]\n",
    "    emb_word_c = glove[word_c]\n",
    "\n",
    "    new_embed = emb_word_b - emb_word_a + emb_word_c\n",
    "    \n",
    "    result = get_top_k(3, new_embed, None, glove.vectors, device, distance=\"cosine\", return_res=True, lookup_token=lambda idx: glove.itos[idx])\n",
    "    result = list(map(str.upper, result))\n",
    "    \n",
    "    if result:\n",
    "        print(f\"{word_a.upper()} is to {word_b.upper()} as {word_c.upper()} is to {result}\")\n",
    "    else:\n",
    "        print(\"Some words in the analogy are not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12300cc4-cdd5-460c-bc5d-bc7b7b2f1fdc",
   "metadata": {},
   "source": [
    "We see that these embeddings perform much better, while also revealing some gender bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced70604",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed45b24-cb51-4938-ab39-2eb9a1ba1512",
   "metadata": {},
   "source": [
    "## **Forecasting with Seq2Seq** <a class=\"anchor\" id=\"app-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563ddc3-5028-4d2c-b184-6aa848eac1e8",
   "metadata": {},
   "source": [
    "We'll be using the [Seq2Seq](https://arxiv.org/pdf/1409.3215) architecture (also known as \"Sutskever model\") to predict the next frames from [Moving MNIST dataset](https://paperswithcode.com/dataset/moving-mnist). Seq2Seq models have been used for sequence-to-sequence translation (hence the name), mostly in LanguageA-to-LanguageB tasks. Check out [this tutorial notebook](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) on german-to-english translation.\n",
    "\n",
    "We'll adapt the Seq2Seq architecture to work on 2D data (frames) instead of tokens, by utilizing a $2D \\rightarrow 1D$ **encoder** that will process all input frames independently, followed by a LSTM network to model the sequence of temporal features, and a final $1D \\rightarrow 2D$ **decoder** that will translate the predicted 1D features to 2D future frames.\n",
    "\n",
    "Check out [these nice visuals](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) on how Seq2Seq works (including adding attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05d30316-2d8a-4dc4-b68c-87aea1057e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy to data/MovingMNIST/mnist_test_seq.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819200096/819200096 [02:27<00:00, 5544025.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MovingMNIST\n",
    "\n",
    "split_ratio = 10\n",
    "\n",
    "transform = lambda x: x / 255.0\n",
    "\n",
    "first_frames = MovingMNIST(root=\"data/\", split=\"train\", download=True, split_ratio=split_ratio, transform=transform)\n",
    "last_frames = MovingMNIST(root=\"data/\", split=\"test\", download=True, split_ratio=split_ratio, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd90fb4-746b-445a-8ce0-60da63429eb7",
   "metadata": {},
   "source": [
    "We'll be using the first 10 frames as input, and our task is to predict the following 10 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5953c338-c446-49f5-a85b-a08f686682df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first frames (known):  torch.Size([16, 10, 1, 64, 64])\n",
      "Shape of last frames (unknown):  torch.Size([16, 10, 1, 64, 64])\n",
      "Value range: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "bs_ = 16\n",
    "\n",
    "print(\"Shape of first frames (known): \", first_frames[:bs_].shape)\n",
    "print(\"Shape of last frames (unknown): \", last_frames[:bs_].shape)\n",
    "print(f\"Value range: [{first_frames[:bs_].min()}, {first_frames[:bs_].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8859ad06-0dda-4d74-88d3-c25dfe1fba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08df0f292d8c4f088193e8aad1efea7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=19), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_frame(frame_number)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "i = np.random.randint(0, len(first_frames))\n",
    "all_frames = np.concatenate((first_frames[i], last_frames[i]), axis=0)\n",
    "\n",
    "def display_frame(frame_number):\n",
    "    plt.imshow(all_frames[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "frame_slider = widgets.IntSlider(min=0, max=all_frames.shape[0] - 1, step=1, description='Frame')\n",
    "widgets.interact(display_frame, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f7a23-214e-4002-a4b6-eee1abd772a3",
   "metadata": {},
   "source": [
    "### Split and Create train/test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cac11fee-a499-4c20-8c2a-88b509b434ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "\n",
    "class CustomMMNIST(Dataset):\n",
    "    def __init__(self, first_fs, last_fs):\n",
    "        self.first_ = first_fs\n",
    "        self.last_ = last_fs\n",
    "\n",
    "        assert len(self.first_) == len(self.last_)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.first_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.first_[idx], self.last_[idx]\n",
    "\n",
    "indexes = list(range(len(first_frames)))\n",
    "train_idx, test_idx = train_test_split(indexes, test_size=0.2, random_state=42)  \n",
    "\n",
    "first_train, first_test = Subset(first_frames, train_idx), Subset(first_frames, test_idx)\n",
    "last_train, last_test = Subset(last_frames, train_idx), Subset(last_frames, test_idx)\n",
    "\n",
    "train_mnist = CustomMMNIST(first_train, last_train)\n",
    "test_mnist = CustomMMNIST(first_test, last_test)\n",
    "\n",
    "train_loader = DataLoader(train_mnist, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_mnist, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "461c25fa-4b91-4608-bec4-7c11b8032195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_mnist), len(test_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463832f-e09d-43cf-9940-c87bc4900b06",
   "metadata": {},
   "source": [
    "### Construct Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b807d-f145-4edd-8cd3-df8a81985236",
   "metadata": {},
   "source": [
    "The usual architecture of Seq2Seq is depicted as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ef791",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/seq2seq.png\" alt=\"description\" width=\"1200\"/>\n",
    "  <figcaption>Seq2Seq architecture. <a href=\"https://blog.suriya.app/2016-12-31-practical-seq2seq/\">Source</a></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352bd3d-af18-4642-b3d7-e14dd4afa24c",
   "metadata": {},
   "source": [
    "We have an RNN encoder that processes the input sequence, followed by another RNN decoder that receives the final states of the encoder and generates the output sequence, element-by-element, using the previously generated output as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ae44a-eafb-40ac-bbbe-c178e81c6332",
   "metadata": {},
   "source": [
    "Before diving into Seq2Seq, we need to define an image encoder that will transform each frame into a feature vector, to be passed to the RNN:\n",
    "\n",
    "$$\\texttt{Frame}_i \\rightarrow \\texttt{ImageEncoder}(\\cdot) \\rightarrow \\texttt{Feature Vector}_i \\rightarrow \\texttt{Seq2Seq}(\\cdot) \\rightarrow \\texttt{ImageDecoder}(\\cdot) \\rightarrow \\texttt{Frame}_{i+1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f584de8-2d39-4db0-994d-117817d44cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "    \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 8, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 4, 4, 0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 4, 4, 0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, out_ch, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 4, 4, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, frames):\n",
    "        \"\"\"\n",
    "        Frames has shape (B', C, H, W), where B' = B * n_frames represents the \"extended batch size\"\n",
    "        \"\"\"\n",
    "        return self.net(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061836a-4ce8-4711-9f08-e6df5bb25e3b",
   "metadata": {},
   "source": [
    "We construct a simple Conv2d encoder, that succesively downsaples the input $64\\times 64$ image into a 1D fature vector with `out_ch` elements. Note that the above architecture is exclusively built for $64\\times 64$ images, with higher resolutions not being reduced to a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13430058-20d8-4707-8dd0-4e22939347e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input:  torch.Size([32, 10, 1, 64, 64])\n",
      "Rearranged input:  torch.Size([320, 1, 64, 64])\n",
      "Original output:  torch.Size([320, 128, 1, 1])\n",
      "Rearranged output:  torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "x, y = train_mnist[:32]\n",
    "bs, t, _, _, _ = x.shape\n",
    "\n",
    "x_ = einops.rearrange(x, \"d1 d2 d3 d4 d5 -> (d1 d2) d3 d4 d5\") # we merge the first two dimensions to process all images at once\n",
    "out_ = ImageEncoder(1, 128)(x_)\n",
    "out = einops.rearrange(out_, \"(b t) c 1 1 -> b t c\", b=bs, t=t) # (b, t, c) is the data format our LSTM will expect\n",
    "\n",
    "y_ = einops.rearrange(y, \"b t c h w -> (b t) c h w\")\n",
    "out_y_ = ImageEncoder(1, 128)(y_)\n",
    "out_y = einops.rearrange(out_y_, \"(b t) c 1 1 -> b t c\", b=bs, t=y.shape[1])\n",
    "\n",
    "print(\"Original input: \", x.shape)\n",
    "print(\"Rearranged input: \", x_.shape)\n",
    "print(\"Original output: \", out_.shape)\n",
    "print(\"Rearranged output: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c84067-4ea0-4c10-93a3-52e49db4ca84",
   "metadata": {},
   "source": [
    "Further, we need to reconstruct frames from abstract features returned by the RNN. Therefore, we need an `ImageDecoder` *somewhat symmetric* to the `ImageEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9e9fab8-5049-4dfc-9f97-11b23784d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(in_ch // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch // 2, in_ch // 2, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_ch // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_ch // 2, in_ch // 4, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(in_ch // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch // 4, in_ch // 4, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_ch // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_ch // 4, in_ch // 8, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(in_ch // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch // 8, in_ch // 8, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_ch // 8),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_ch // 8, out_ch, 1, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features - tensor of shape (B', C, 1, 1). \n",
    "        \"\"\"\n",
    "        return self.net(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "127bf3c8-03ab-4ade-9c24-0912d37047e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded frame shape:  torch.Size([320, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "decoded = ImageDecoder(128, 1)(einops.rearrange(out, \"b t c -> (b t) c 1 1\"))\n",
    "\n",
    "print(\"Decoded frame shape: \", decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7272fe-028f-411f-85d5-59b52cda7615",
   "metadata": {},
   "source": [
    "The temporal features produced by `ImageDecoder` will be fed to a Seq2Seq architecture, which will return a similar tensor for the predicted last frames. Let's design separately the encoder and decoder parts of seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b88e07d-06b3-4162-a0e0-d983754ad17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, vector_features):\n",
    "        outputs, (hidden, cell) = self.rnn(vector_features)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim  # output_dim should be = hidden dim of image features from ImageEncoder\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        if hidden_dim != output_dim:\n",
    "            self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc_out = nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, in_token, hidden, cell):\n",
    "        output, (hidden, cell) = self.rnn(in_token, (hidden, cell))\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6dbc104-de4f-4641-9d63-3c301891a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden/cell states:  torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=0 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=1 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=2 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=3 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=4 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=5 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=6 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=7 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=8 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n",
      "step=9 Decoder output/hidden/cell states:  torch.Size([32, 1, 128]) torch.Size([4, 32, 128]) torch.Size([4, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "h, c = Encoder(out.shape[-1], 128, 4)(out)\n",
    "print(\"Encoder hidden/cell states: \", h.shape, c.shape) # (num_lstm_layers, batch_size, hidden_dim)\n",
    "\n",
    "dec = Decoder(128, 128, 128, 4)\n",
    "in_token = torch.zeros((bs, 1, out_y.shape[-1]))\n",
    "\n",
    "# we'll need to iterate over the target sequence step-by-step, compute the current output, and fed it as input in the next step\n",
    "for i in range(y.shape[1]):\n",
    "    out_d, h, c = dec(in_token, h, c)\n",
    "    print(f\"step={i} Decoder output/hidden/cell states: \", out_d.shape, h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f7434-1478-4998-b847-904e2a1437a8",
   "metadata": {},
   "source": [
    "Combine an `Encoder` and a `Decoder` instance into a `Seq2Seq` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "019a1e7b-8535-4ef0-bcd0-9829fb82b024",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, dropout=0.1, learnable_start_token=True):        \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, input_dim, n_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(input_dim, input_dim, input_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # We need a START token. We can either set it to 0 or learn it.\n",
    "        self.start_token = nn.Parameter(torch.zeros((1, 1, input_dim)), requires_grad=learnable_start_token)\n",
    "    \n",
    "    def forward(self, source, target, teacher_forcing_ratio):\n",
    "        \n",
    "        batch_size = target.shape[0]\n",
    "        trg_length = target.shape[1]\n",
    "\n",
    "        outputs = torch.zeros_like(target).to(source.device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Duplicate start token for the current batch\n",
    "        input = einops.repeat(self.start_token, \"1 1 x -> rep 1 x\", rep=batch_size)\n",
    "        # input = source[:, -1].unsqueeze(1)\n",
    "        \n",
    "        for t in range(trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[:, t] = output[:, 0]\n",
    "            \n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            input = target[:, t].unsqueeze(1) if teacher_force else output\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544c0de-651e-43c9-a301-d7ca78792f14",
   "metadata": {},
   "source": [
    "In the above, we have defined a `teacher_forcing_ratio`, which is a number in `[0, 1]` representing the probability that in the next step we'll feed as input the true target frame, otherwise we'll feed the previously generated frame. This is a helpful trick during training, otherwise the forward process might get stuck in unrealistic solutions if the previous timestep wasn't good enough, hence a slower optimization process.\n",
    "\n",
    "**Obiously, during inferece `teacher_forcing_ratio = 0` since we don't practically have access to the future frames.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feddae4a-7cd0-4f22-80ee-189e58cea0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq output:  torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "ss = Seq2Seq(128, 4)\n",
    "print(\"Seq2Seq output: \", ss(out, out_y, 0.1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00638988-2f45-4f01-95ca-8dd9aaf436ca",
   "metadata": {},
   "source": [
    "Finally, let's wrap everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d99871a6-72ef-44cc-a9ac-b0540cdfd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastModel(nn.Module):\n",
    "    def __init__(self, in_ch, inner_dim, n_lstm_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.inner_dim = inner_dim\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.drop = dropout\n",
    "\n",
    "        self.im_enc = ImageEncoder(in_ch, inner_dim)\n",
    "        self.im_dec = ImageDecoder(inner_dim, in_ch)\n",
    "\n",
    "        self.seq2seq = Seq2Seq(inner_dim, n_lstm_layers, dropout)\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio):\n",
    "\n",
    "        b = source.shape[0]\n",
    "        ts, tt = source.shape[1], target.shape[1]\n",
    "\n",
    "        # Encode input frames\n",
    "        source_ = einops.rearrange(source, \"b t c h w -> (b t) c h w\")\n",
    "        source_ = self.im_enc(source_)\n",
    "        source_ = einops.rearrange(source_, \"(b t) c 1 1 -> b t c\", b=b, t=ts)\n",
    "\n",
    "        # Encode target frames\n",
    "        target_ = einops.rearrange(target, \"b t c h w -> (b t) c h w\")\n",
    "        target_ = self.im_enc(target_)\n",
    "        target_ = einops.rearrange(target_, \"(b t) c 1 1 -> b t c\", b=b, t=tt)\n",
    "\n",
    "        # Apply Seq2Seq\n",
    "        ss_pred = self.seq2seq(source_, target_, teacher_forcing_ratio)\n",
    "        ss_pred_ = einops.rearrange(ss_pred, \"b t c -> (b t) c 1 1\")\n",
    "\n",
    "        # Reconstruct next frames\n",
    "        frame_pred = self.im_dec(ss_pred_)\n",
    "        frame_pred = einops.rearrange(frame_pred, \"(b t) c h w -> b t c h w\", b=b, t=tt)\n",
    "        \n",
    "        return frame_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa97c12-5b77-4c85-9995-fba56d928d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([32, 10, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "out = ForecastModel(1, 32, 4, 0.1)(x, y, 0.1)\n",
    "\n",
    "print(\"Output shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b482f-add5-4d2d-8080-56ce27d06fb3",
   "metadata": {},
   "source": [
    "### Train Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b0acfb5-1855-46f9-8456-6870be75f853",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_ch = 1\n",
    "inner_dim = 128\n",
    "n_lstm_layers = 2\n",
    "dropout = 0.3\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "folder_path = \"models/mmnist/\"\n",
    "file_name = \"model.pth\"\n",
    "\n",
    "model = ForecastModel(in_ch, inner_dim, n_lstm_layers, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb630914",
   "metadata": {},
   "source": [
    "Instead of standard $\\ell_1$ or $\\ell_2$ losses we can use the `BCEWithLogitsLoss` for each predicted pixel - this is possible **only because images are binary, i.e. targets are \\{0, 1\\}**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f951cb67-fc4d-416a-9ec4-95c1252f0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c524185-c2d1-439f-8702-2953d2feaaba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [07:58,  1.92s/it]\n",
      "63it [00:52,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: train_loss=0.297975844681263 test_loss=0.18690356517594958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [08:20,  2.00s/it]\n",
      "63it [00:58,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: train_loss=0.13593635672330856 test_loss=0.14365514001202961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [07:46,  1.86s/it]\n",
      "63it [00:51,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: train_loss=0.1146246946156025 test_loss=0.1313526084025701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [08:48,  2.12s/it]\n",
      "63it [00:55,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: train_loss=0.10802784487605095 test_loss=0.12533846119093517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:00,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(torch_utils)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop_forecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/BIOSINF/BIOSINF_IA2/L4/torch_utils.py:195\u001b[0m, in \u001b[0;36mtrain_loop_forecast\u001b[0;34m(model, train_loader, optimizer, loss, epochs, teacher_forcing_ratio, test_loader, device, folder_path, file_name, print_frequency)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 195\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43miterate_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/BIOSINF/BIOSINF_IA2/L4/torch_utils.py:150\u001b[0m, in \u001b[0;36miterate_forecast\u001b[0;34m(model, dataloader, optimizer, loss_fn, teacher_forcing_ratio, is_training, device)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    149\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Cumulative loss for reporting\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ia2/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ia2/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch_utils\n",
    "importlib.reload(torch_utils)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_losses, test_losses = torch_utils.train_loop_forecast(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    teacher_forcing_ratio,\n",
    "    test_loader=test_loader, \n",
    "    device=device, \n",
    "    folder_path=folder_path, \n",
    "    file_name=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d34ce10f-38ec-4ef5-918f-c54288c8c5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = -5\n",
    "\n",
    "pred = model(x[i].unsqueeze(0).to(device), y[i].unsqueeze(0).to(device), 0)\n",
    "pred = nn.functional.sigmoid(pred)\n",
    "pred = pred.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "all_frames_pred = np.concatenate((x[i], pred), axis=0)\n",
    "all_frames = np.concatenate((x[i], y[i]), axis=0)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(all_frames.shape[0]):\n",
    "    plt.subplot(1, all_frames.shape[0], i+1)\n",
    "    plt.imshow(all_frames[i].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(all_frames_pred.shape[0]):\n",
    "    plt.subplot(1, all_frames_pred.shape[0], i+1)\n",
    "    plt.imshow(all_frames_pred[i].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3262bcba-7618-4975-bf8b-9767fdd02f9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a14cea61db4b43895b7112e4bccc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=19), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_frame(frame_number)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_frame(frame_number):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.imshow(all_frames_pred[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"GT\")\n",
    "    plt.imshow(all_frames[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "frame_slider = widgets.IntSlider(min=0, max=all_frames_pred.shape[0] - 1, step=1, description='Frame')\n",
    "widgets.interact(display_frame, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38788037",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- As we go deeper into the temporal dimension, the information regarding class digit fades\n",
    "- The dynamics remain well-predicted, meaning that the `Seq2Seq` model successfully retains & uses information from previous frames\n",
    "- The `ImageEncoder` and `ImageDecoder` could share some information in order for the digit structure to remain intact after decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc69d5a",
   "metadata": {},
   "source": [
    "### (Optional) Seq2Seq + Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cac4c3-0db3-48af-aad2-94a5d8c3a035",
   "metadata": {},
   "source": [
    "Let's add a `MultiheadAttention` attention module over the output temporal features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe80eac",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/seq2seq_at.png\" alt=\"description\" width=\"1000\"/>\n",
    "  <figcaption>Seq2Seq + Self-Attention. <a href=\"https://blog.suriya.app/2016-12-31-practical-seq2seq/\">Source</a></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f35a859-79b0-4132-a48d-7a707d777002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch_utils\n",
    "importlib.reload(torch_utils)\n",
    "\n",
    "class ForecastModelAtt(nn.Module):\n",
    "    def __init__(self, in_ch, inner_dim, n_lstm_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.inner_dim = inner_dim\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.drop = dropout\n",
    "\n",
    "        self.im_enc = ImageEncoder(in_ch, inner_dim)\n",
    "        self.im_dec = ImageDecoder(inner_dim, in_ch)\n",
    "\n",
    "        self.seq2seq = Seq2Seq(inner_dim, n_lstm_layers, dropout)\n",
    "\n",
    "        # this actually doesn't replicate the input to each head, but rather divides the input into n_heads and applies attention separately on each part\n",
    "        # check the documentation\n",
    "        self.att = nn.MultiheadAttention(inner_dim, num_heads=4, batch_first=True, dropout=0.5)\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio):\n",
    "\n",
    "        b = source.shape[0]\n",
    "        ts, tt = source.shape[1], target.shape[1]\n",
    "\n",
    "        # Encode input frames\n",
    "        source_ = einops.rearrange(source, \"b t c h w -> (b t) c h w\")\n",
    "        source_ = self.im_enc(source_)\n",
    "        source_ = einops.rearrange(source_, \"(b t) c 1 1 -> b t c\", b=b, t=ts)\n",
    "\n",
    "        # Encode target frames\n",
    "        target_ = einops.rearrange(target, \"b t c h w -> (b t) c h w\")\n",
    "        target_ = self.im_enc(target_)\n",
    "        target_ = einops.rearrange(target_, \"(b t) c 1 1 -> b t c\", b=b, t=tt)\n",
    "\n",
    "        # Apply Seq2Seq\n",
    "        ss_pred = self.seq2seq(source_, target_, teacher_forcing_ratio)\n",
    "\n",
    "        # Apply attention\n",
    "        ss_pred, att_weights = self.att(ss_pred, ss_pred, ss_pred)\n",
    "        \n",
    "        ss_pred_ = einops.rearrange(ss_pred, \"b t c -> (b t) c 1 1\")\n",
    "\n",
    "        # Reconstruct next frames\n",
    "        frame_pred = self.im_dec(ss_pred_)\n",
    "        frame_pred = einops.rearrange(frame_pred, \"(b t) c h w -> b t c h w\", b=b, t=tt)\n",
    "        \n",
    "        return frame_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1e348-34f9-48ae-aa95-de732e5d3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 1\n",
    "inner_dim = 128\n",
    "n_lstm_layers = 2\n",
    "dropout = 0.2\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "folder_path = \"models/mmnist_att/\"\n",
    "file_name = \"model.pth\"\n",
    "\n",
    "model_att = ForecastModelAtt(in_ch, inner_dim, n_lstm_layers, dropout)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_att.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_losses, test_losses = torch_utils.train_loop_forecast(\n",
    "    model_att, train_loader, optimizer, criterion, epochs, teacher_forcing_ratio,\n",
    "    test_loader=test_loader, device=device, folder_path=folder_path, file_name=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3ee609f-29cb-4c56-bf61-e7eb244efa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f305fb4ed8bc4efea6ba79bcd1ad7119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=19), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_frame(frame_number)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = -5\n",
    "\n",
    "pred = model(x[i].unsqueeze(0).to(device), y[i].unsqueeze(0).to(device), 0)\n",
    "pred = nn.functional.sigmoid(pred)\n",
    "pred = pred.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "pred_att = model_att(x[i].unsqueeze(0).to(device), y[i].unsqueeze(0).to(device), 0)\n",
    "pred_att = nn.functional.sigmoid(pred_att)\n",
    "pred_att = pred_att.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "all_frames_pred_att = np.concatenate((x[i], pred_att), axis=0)\n",
    "all_frames_pred = np.concatenate((x[i], pred), axis=0)\n",
    "all_frames = np.concatenate((x[i], y[i]), axis=0)\n",
    "\n",
    "\n",
    "def display_frame(frame_number):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.imshow(all_frames_pred[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Predicted Self-Att\")\n",
    "    plt.imshow(all_frames_pred_att[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"GT\")\n",
    "    plt.imshow(all_frames[frame_number].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "frame_slider = widgets.IntSlider(min=0, max=all_frames_pred.shape[0] - 1, step=1, description='Frame')\n",
    "widgets.interact(display_frame, frame_number=frame_slider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
